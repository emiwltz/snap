# SNAP Experiment Configuration

experiment:
  name: SNAP_POC_v1
  version: "1.0.0"
  description: "Psychometric benchmark for LLM stability and contextual sensitivity"

  # Experimental design variables
  variables:
    items:
      moral: [M01, M02, M03, M04, M05]
      personality: [P01, P02, P03, P04, P05]
    paraphrases: [P1, P2, P3]
    system_prompts: [NEU, DIR, PER, ABS]
    temperatures: [0.0, 0.5, 1.0]
    contexts:
      moral: [C0, C1]      # C0 = baseline, C1 = dilemma context
      personality: [C0]     # No context variation for personality items
    runs: 3

  # Pilot mode configuration (subset for testing)
  pilot:
    models: [claude-haiku, mistral-large, grok-2]
    items:
      moral: [M01, M02]
      personality: [P01, P02]
    runs: 2
    # Estimated calls: 3 models * (2+2) items * 3 paraphrases * 4 prompts * 3 temps * (avg 1.5 contexts) * 2 runs
    # = 3 * 4 * 3 * 4 * 3 * 1.5 * 2 = ~1,296 calls (rough estimate)

  # Full mode configuration
  full:
    models: all  # Uses all models from models.yaml
    # Estimated calls: 10 models * 10 items * 3 paraphrases * 4 prompts * 3 temps * (avg 1.5 contexts) * 3 runs
    # = 10 * 10 * 3 * 4 * 3 * 1.5 * 3 = 16,200 calls

  # API configuration
  api:
    max_retries: 3
    retry_delay_seconds: 1.0
    retry_multiplier: 2.0
    timeout_seconds: 30
    rate_limit_rpm: 60
    rate_limit_burst: 10
    cache_enabled: true
    cache_ttl_days: 7

  # Response configuration
  response:
    max_tokens: 150
    expected_format: "Score: [1-7]\nJustification: [optional text]"

  # Checkpointing
  checkpoint:
    enabled: true
    interval_calls: 100
    directory: "data/checkpoints"

  # Logging
  logging:
    level: INFO
    directory: "data/logs"
    format: "{time:YYYY-MM-DD HH:mm:ss} | {level} | {message}"
